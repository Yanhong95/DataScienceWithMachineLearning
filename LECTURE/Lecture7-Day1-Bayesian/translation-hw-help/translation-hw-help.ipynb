{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 20 February 2020</div>\n",
    "\n",
    "### Professor helping you with your homework \n",
    "\n",
    "I want *everyone* to do this homework. Everyine should be in a team. No solos. Translation should be from either **hindi** or **chinese**, to **english**. That includes non-native speakers, americans, and greeks! The goal is to see how the **HMM** model improves on the **MFC** model.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"https://i.imgur.com/k4APBMw.gif\" width=400 />\n",
    "</center>\n",
    "\n",
    "Install ntlk!\n",
    "```(python)\n",
    "pip install --user -U nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bug in HMM POS Tagger\n",
    "\n",
    "Use `end_tag = [i[len(i)-1] for i in data.Y]` instead of `end_tag = [i[len(i)-2] for i in data.Y]`. Let me know if that fixes your bug or not!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese (中文)\n",
    "\n",
    "Word segmentation is a problem in chinese. There is no white space like in hindi and english!\n",
    "A few other installs:\n",
    "```(python)\n",
    "pip install jieba\n",
    "pip install pinyin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example： 獨立音樂需要大家一起來推廣，歡迎加入我們的行列！\n"
     ]
    }
   ],
   "source": [
    "sentence = \"獨立音樂需要大家一起來推廣，歡迎加入我們的行列！\"\n",
    "print (\"Example：\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\Dino\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.110 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default/Accurate Mode:獨立/ 音樂/ 需要/ 大家/ 一起/ 來/ 推廣/ ，/ 歡迎/ 加入/ 我們/ 的/ 行列/ ！\n"
     ]
    }
   ],
   "source": [
    "words = jieba.cut(sentence, cut_all=False)\n",
    "print(\"Default/Accurate Mode:\" + \"/ \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = '''請記住我 雖然再見必須說\n",
    "請記住我 眼淚不要墜落\n",
    "我雖然要離你遠去 你住在我心底\n",
    "在每個分離的夜裡 為你唱一首歌\n",
    "請記住我 雖然我要去遠方\n",
    "請記住我 當聽見吉他的悲傷\n",
    "這就是我跟你在一起唯一的憑據\n",
    "直到我再次擁抱你 請記住我\n",
    "\n",
    "你閉上眼睛音樂就會響起 不停的愛就永不會流失\n",
    "你閉上眼睛音樂就會響起 要不停的愛\n",
    "\n",
    "請記住我 雖然再見必須說\n",
    "請記住我 眼淚不要墜落\n",
    "我雖然要離你遠去 你住在我心底\n",
    "在每個分離的夜裡 為你唱一首歌\n",
    "請記住我 我即將會消失\n",
    "請記住我 我們的愛不會消失\n",
    "我用我的辦法跟你一起不離不棄\n",
    "直到我再次擁抱你 請記住我'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_3 = jieba.cut(lyrics, cut_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "請/記住/我/ /雖然/再/見/必須/說/\n",
      "/請/記住/我/ /眼淚/不要/墜落/\n",
      "/我/雖然/要/離/你/遠/去/ /你/住/在/我/心底/\n",
      "/在/每個/分離/的/夜裡/ /為/你/唱/一首歌/\n",
      "/請/記住/我/ /雖然/我/要/去/遠方/\n",
      "/請/記住/我/ /當聽/見/吉他/的/悲傷/\n",
      "/這/就是/我/跟/你/在/一起/唯一/的/憑/據/\n",
      "/直到/我/再次/擁抱/你/ /請/記住/我/\n",
      "/\n",
      "/你/閉上/眼睛/音樂/就/會響/起/ /不停/的/愛就/永不/會/流失/\n",
      "/你/閉上/眼睛/音樂/就/會響/起/ /要/不停/的/愛/\n",
      "/\n",
      "/請/記住/我/ /雖然/再/見/必須/說/\n",
      "/請/記住/我/ /眼淚/不要/墜落/\n",
      "/我/雖然/要/離/你/遠/去/ /你/住/在/我/心底/\n",
      "/在/每個/分離/的/夜裡/ /為/你/唱/一首歌/\n",
      "/請/記住/我/ /我/即/將會/消失/\n",
      "/請/記住/我/ /我們/的/愛不會/消失/\n",
      "/我用/我/的/辦法/跟/你/一起/不離/不棄/\n",
      "/直到/我/再次/擁抱/你/ /請/記住/我/"
     ]
    }
   ],
   "source": [
    "for x in words_3:\n",
    "    print(x, end='/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return words with Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_token = jieba.tokenize('''你閉上眼睛音樂就會響起 不停的愛就永不會流失\n",
    "你閉上眼睛音樂就會響起 要不停的愛''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: 你 \t\t start: 0 \t\t end: 1\n",
      "word: 閉上 \t\t start: 1 \t\t end: 3\n",
      "word: 眼睛 \t\t start: 3 \t\t end: 5\n",
      "word: 音樂 \t\t start: 5 \t\t end: 7\n",
      "word: 就 \t\t start: 7 \t\t end: 8\n",
      "word: 會響 \t\t start: 8 \t\t end: 10\n",
      "word: 起 \t\t start: 10 \t\t end: 11\n",
      "word:   \t\t start: 11 \t\t end: 12\n",
      "word: 不停 \t\t start: 12 \t\t end: 14\n",
      "word: 的 \t\t start: 14 \t\t end: 15\n",
      "word: 愛就 \t\t start: 15 \t\t end: 17\n",
      "word: 永不 \t\t start: 17 \t\t end: 19\n",
      "word: 會 \t\t start: 19 \t\t end: 20\n",
      "word: 流失 \t\t start: 20 \t\t end: 22\n",
      "word: \n",
      " \t\t start: 22 \t\t end: 23\n",
      "word: 你 \t\t start: 23 \t\t end: 24\n",
      "word: 閉上 \t\t start: 24 \t\t end: 26\n",
      "word: 眼睛 \t\t start: 26 \t\t end: 28\n",
      "word: 音樂 \t\t start: 28 \t\t end: 30\n",
      "word: 就 \t\t start: 30 \t\t end: 31\n",
      "word: 會響 \t\t start: 31 \t\t end: 33\n",
      "word: 起 \t\t start: 33 \t\t end: 34\n",
      "word:   \t\t start: 34 \t\t end: 35\n",
      "word: 要 \t\t start: 35 \t\t end: 36\n",
      "word: 不停 \t\t start: 36 \t\t end: 38\n",
      "word: 的 \t\t start: 38 \t\t end: 39\n",
      "word: 愛 \t\t start: 39 \t\t end: 40\n"
     ]
    }
   ],
   "source": [
    "for x in word_token:\n",
    "    print('word: %s \\t\\t start: %d \\t\\t end: %d' % (x[0],x[1],x[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = jieba.analyse.extract_tags(lyrics, topK=10, withWeight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('記住', 1.7325750004202898),\n",
       " ('雖然', 0.8662875002101449),\n",
       " ('必須', 0.34651500008405794),\n",
       " ('眼淚', 0.34651500008405794),\n",
       " ('墜落', 0.34651500008405794),\n",
       " ('每個', 0.34651500008405794),\n",
       " ('分離', 0.34651500008405794),\n",
       " ('夜裡', 0.34651500008405794),\n",
       " ('擁抱', 0.34651500008405794),\n",
       " ('閉上', 0.34651500008405794)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is based on the trained idf comes along with the jieba library. In practice, we might want to use different idf in different semantics environment.\n",
    "\n",
    "If you want to learn the idf vector for specific corpus, try using scikit-learn sklearn.feature_extraction.text.TfidfVectorizer and then load it with jieba.analyse.set_idf_path(file_name).\n",
    "\n",
    "Same function available for stop words: `jieba.analyse.set_stop_words(file_name)`\n",
    "\n",
    "TextRank is an algorithm developed by Mihalcea & Tarau (2004)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('記住', 1.0),\n",
       " ('直到', 0.6734362162738576),\n",
       " ('眼淚', 0.6473033801750212),\n",
       " ('擁抱', 0.6335844833599963),\n",
       " ('不離', 0.5800086245889862),\n",
       " ('響起', 0.4873857768910344),\n",
       " ('音樂', 0.4852445385958958),\n",
       " ('眼睛', 0.4828605624050096),\n",
       " ('聽見', 0.42734618921539375),\n",
       " ('不棄', 0.37081958664085696),\n",
       " ('墜落', 0.3365544925169352),\n",
       " ('吉他', 0.24559953407618335),\n",
       " ('辦法', 0.2277016847505973),\n",
       " ('消失', 0.1997237834777403)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.analyse.textrank(lyrics, withWeight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nǐ hǎo\n"
     ]
    }
   ],
   "source": [
    "import pinyin\n",
    "print (pinyin.get('你 好'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, `pinyin` can also translate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you (informal, as opposed to courteous 您[nin2])']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pinyin.cedict\n",
    "pinyin.cedict.translate_word('你')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello!', 'Hi!', 'How are you?']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinyin.cedict.translate_word('你好')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['你', ['you (informal, as opposed to courteous 您[nin2])']],\n",
       " ['你好', ['Hello!', 'Hi!', 'How are you?']],\n",
       " ['好', ['to be fond of', 'to have a tendency to', 'to be prone to']]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pinyin.cedict.all_phrase_translations('你好'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return words with Parts of Speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jieba import posseg as pseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_pseg = pseg.cut('''你閉上眼睛音樂就會響起 不停的愛就永不會流失\n",
    "你閉上眼睛音樂就會響起 要不停的愛''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你/r\n",
      "閉/v\n",
      "上/f\n",
      "眼睛/n\n",
      "音樂/n\n",
      "就/d\n",
      "會/v\n",
      "響起/v\n",
      " /x\n",
      "不停/d\n",
      "的/uj\n",
      "愛/v\n",
      "就/d\n",
      "永不/d\n",
      "會/zg\n",
      "流失/v\n",
      "\n",
      "/x\n",
      "你/r\n",
      "閉/v\n",
      "上/f\n",
      "眼睛/n\n",
      "音樂/n\n",
      "就/d\n",
      "會/v\n",
      "響起/v\n",
      " /x\n",
      "要/v\n",
      "不停/d\n",
      "的/uj\n",
      "愛/n\n"
     ]
    }
   ],
   "source": [
    "for x in words_pseg:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindi (हिंदी)\n",
    "\n",
    "You need to run this first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package indian to\n",
      "[nltk_data]     C:\\Users\\Dino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\indian.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('indian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "वाशिंगटन: दुनिया के सबसे शक्तिशाली देश के राष्ट्रपति बराक ओबामा ने प्रधानमंत्री नरेंद्र मोदी के संदर्भ में 'टाइम' पत्रिका में लिखा, \"नरेंद्र मोदी ने अपने बाल्यकाल में अपने परिवार की सहायता करने के लिए अपने पिता की चाय बेचने में मदद की थी। आज वह दुनिया के सबसे बड़े लोकतंत्र के नेता हैं और गरीबी से प्रधानमंत्री तक की उनकी जिंदगी की कहानी भारत के उदय की गतिशीलता और क्षमता को परिलक्षित करती है।\n"
     ]
    }
   ],
   "source": [
    "t = '''वाशिंगटन: दुनिया के सबसे शक्तिशाली देश के राष्ट्रपति बराक ओबामा ने प्रधानमंत्री नरेंद्र मोदी के संदर्भ में 'टाइम' पत्रिका में लिखा, \"नरेंद्र मोदी ने अपने बाल्यकाल में अपने परिवार की सहायता करने के लिए अपने पिता की चाय बेचने में मदद की थी। आज वह दुनिया के सबसे बड़े लोकतंत्र के नेता हैं और गरीबी से प्रधानमंत्री तक की उनकी जिंदगी की कहानी भारत के उदय की गतिशीलता और क्षमता को परिलक्षित करती है।'''\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['वाशिंगटन', ':', 'दुनिया', 'के', 'सबसे', 'शक्तिशाली', 'देश', 'के', 'राष्ट्रपति', 'बराक', 'ओबामा', 'ने', 'प्रधानमंत्री', 'नरेंद्र', 'मोदी', 'के', 'संदर्भ', 'में', \"'\", 'टाइम', \"'\", 'पत्रिका', 'में', 'लिखा', ',', '``', 'नरेंद्र', 'मोदी', 'ने', 'अपने', 'बाल्यकाल', 'में', 'अपने', 'परिवार', 'की', 'सहायता', 'करने', 'के', 'लिए', 'अपने', 'पिता', 'की', 'चाय', 'बेचने', 'में', 'मदद', 'की', 'थी।', 'आज', 'वह', 'दुनिया', 'के', 'सबसे', 'बड़े', 'लोकतंत्र', 'के', 'नेता', 'हैं', 'और', 'गरीबी', 'से', 'प्रधानमंत्री', 'तक', 'की', 'उनकी', 'जिंदगी', 'की', 'कहानी', 'भारत', 'के', 'उदय', 'की', 'गतिशीलता', 'और', 'क्षमता', 'को', 'परिलक्षित', 'करती', 'है।']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('refuse', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('permit', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('refuse', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('মহিষের', 'NN'), ('সন্তান', 'NN'), (':', 'SYM'), ...]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.indian.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('वाशिंगटन', 'NN'),\n",
       " (':', ':'),\n",
       " ('दुनिया', 'JJ'),\n",
       " ('के', 'NNP'),\n",
       " ('सबसे', 'NNP'),\n",
       " ('शक्तिशाली', 'NNP'),\n",
       " ('देश', 'NNP'),\n",
       " ('के', 'NNP'),\n",
       " ('राष्ट्रपति', 'NNP'),\n",
       " ('बराक', 'NNP'),\n",
       " ('ओबामा', 'NNP'),\n",
       " ('ने', 'NNP'),\n",
       " ('प्रधानमंत्री', 'NNP'),\n",
       " ('नरेंद्र', 'NNP'),\n",
       " ('मोदी', 'NNP'),\n",
       " ('के', 'NNP'),\n",
       " ('संदर्भ', 'NNP'),\n",
       " ('में', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('टाइम', 'NNP'),\n",
       " (\"'\", 'POS'),\n",
       " ('पत्रिका', 'NN'),\n",
       " ('में', 'NNP'),\n",
       " ('लिखा', 'NNP'),\n",
       " (',', ','),\n",
       " ('``', '``'),\n",
       " ('नरेंद्र', 'JJ'),\n",
       " ('मोदी', 'NN'),\n",
       " ('ने', 'NNP'),\n",
       " ('अपने', 'NNP'),\n",
       " ('बाल्यकाल', 'NNP'),\n",
       " ('में', 'NNP'),\n",
       " ('अपने', 'NNP'),\n",
       " ('परिवार', 'NNP'),\n",
       " ('की', 'NNP'),\n",
       " ('सहायता', 'NNP'),\n",
       " ('करने', 'NNP'),\n",
       " ('के', 'NNP'),\n",
       " ('लिए', 'NNP'),\n",
       " ('अपने', 'NNP'),\n",
       " ('पिता', 'NNP'),\n",
       " ('की', 'NNP'),\n",
       " ('चाय', 'NNP'),\n",
       " ('बेचने', 'NNP'),\n",
       " ('में', 'NNP'),\n",
       " ('मदद', 'NNP'),\n",
       " ('की', 'NNP'),\n",
       " ('थी।', 'NNP'),\n",
       " ('आज', 'NNP'),\n",
       " ('वह', 'NNP'),\n",
       " ('दुनिया', 'NNP'),\n",
       " ('के', 'NNP'),\n",
       " ('सबसे', 'NNP'),\n",
       " ('बड़े', 'NNP'),\n",
       " ('लोकतंत्र', 'NNP'),\n",
       " ('के', 'NNP'),\n",
       " ('नेता', 'NNP'),\n",
       " ('हैं', 'NNP'),\n",
       " ('और', 'NNP'),\n",
       " ('गरीबी', 'NNP'),\n",
       " ('से', 'NNP'),\n",
       " ('प्रधानमंत्री', 'NNP'),\n",
       " ('तक', 'NNP'),\n",
       " ('की', 'NNP'),\n",
       " ('उनकी', 'NNP'),\n",
       " ('जिंदगी', 'NNP'),\n",
       " ('की', 'NNP'),\n",
       " ('कहानी', 'NNP'),\n",
       " ('भारत', 'NNP'),\n",
       " ('के', 'NNP'),\n",
       " ('उदय', 'NNP'),\n",
       " ('की', 'NNP'),\n",
       " ('गतिशीलता', 'NNP'),\n",
       " ('और', 'NNP'),\n",
       " ('क्षमता', 'NNP'),\n",
       " ('को', 'NNP'),\n",
       " ('परिलक्षित', 'NNP'),\n",
       " ('करती', 'NNP'),\n",
       " ('है।', 'NN')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi_text = word_tokenize(t)\n",
    "nltk.pos_tag(hindi_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... Not too sure this is working. Probably need to do this before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import tnt\n",
    "from nltk.corpus import indian\n",
    "train_data = indian.tagged_sents('hindi.pos')\n",
    "tnt_pos_tagger = tnt.TnT()\n",
    "tnt_pos_tagger.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('वाशिंगटन', 'Unk'), (':', 'SYM'), ('दुनिया', 'NN'), ('के', 'PREP'), ('सबसे', 'INTF'), ('शक्तिशाली', 'Unk'), ('देश', 'NN'), ('के', 'PREP'), ('राष्ट्रपति', 'NNC'), ('बराक', 'NNP'), ('ओबामा', 'Unk'), ('ने', 'PREP'), ('प्रधानमंत्री', 'NN'), ('नरेंद्र', 'NNPC'), ('मोदी', 'NNP'), ('के', 'PREP'), ('संदर्भ', 'Unk'), ('में', 'PREP'), (\"'\", 'SYM'), ('टाइम', 'Unk'), (\"'\", 'SYM'), ('पत्रिका', 'Unk'), ('में', 'PREP'), ('लिखा', 'VFM'), (',', 'PUNC'), ('``', 'Unk'), ('नरेंद्र', 'NNPC'), ('मोदी', 'NNP'), ('ने', 'PREP'), ('अपने', 'PRP'), ('बाल्यकाल', 'Unk'), ('में', 'PREP'), ('अपने', 'PRP'), ('परिवार', 'NN'), ('की', 'PREP'), ('सहायता', 'NVB'), ('करने', 'VNN'), ('के', 'PREP'), ('लिए', 'PREP'), ('अपने', 'PRP'), ('पिता', 'NN'), ('की', 'PREP'), ('चाय', 'Unk'), ('बेचने', 'VNN'), ('में', 'PREP'), ('मदद', 'NN'), ('की', 'PREP'), ('थी।', 'Unk'), ('आज', 'NN'), ('वह', 'PRP'), ('दुनिया', 'NN'), ('के', 'PREP'), ('सबसे', 'INTF'), ('बड़े', 'Unk'), ('लोकतंत्र', 'NN'), ('के', 'PREP'), ('नेता', 'NN'), ('हैं', 'VFM'), ('और', 'CC'), ('गरीबी', 'NN'), ('से', 'PREP'), ('प्रधानमंत्री', 'NN'), ('तक', 'PREP'), ('की', 'PREP'), ('उनकी', 'PRP'), ('जिंदगी', 'Unk'), ('की', 'PREP'), ('कहानी', 'Unk'), ('भारत', 'NNP'), ('के', 'PREP'), ('उदय', 'Unk'), ('की', 'PREP'), ('गतिशीलता', 'Unk'), ('और', 'CC'), ('क्षमता', 'Unk'), ('को', 'PREP'), ('परिलक्षित', 'Unk'), ('करती', 'VFM'), ('है।', 'Unk')]\n"
     ]
    }
   ],
   "source": [
    "tagged_words = (tnt_pos_tagger.tag(nltk.word_tokenize(t)))\n",
    "print(tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahh! Much better!\n",
    "\n",
    "However, nltk data seems not to be complete: We get the tag `Unk` frequently, e.g. while tagging the words ex: (‘वाशिंग’, ‘Unk’), (‘मशीन’, ‘Unk’). Read about some work on this [here](https://github.com/pemagrg1/Hindi-POS-Tagging-and-Keyword-Extraction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLTK \n",
    "\n",
    "Although I don't think you need it, `cltk` is the ***classical*** language toolkit. Includes *Chinese*, *Greek*, and *Hindi*! Read about it [here](http://docs.cltk.org/en/latest/hindi.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "c = CorpusImporter('hindi')\n",
    "c.list_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to download [this](https://github.com/cltk/hindi_text_ltrc) to obtain hindi texts, like Mahatma Ghandi's speeches.\n",
    "\n",
    ">Did I tell you my father introduced me to [Indira Ghandhi](https://en.wikipedia.org/wiki/Indira_Gandhi) when I was a child, just a few weeks before she was gunned down? \n",
    "\n",
    "The below does not seem to work. What am I doing wrong? Anyway, probably not that important, `nltk` seems to be working ok with hindi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "import os\n",
    "root = os.path.expanduser('~')\n",
    "# (mac) hindi_corpus = os.path.join(root,'cltk_data/hindi/text/hindi_text_ltrc')\n",
    "hindi_corpus = os.path.join('d:/','user/docs/NU/_Info6105/hindi_text_ltrc-master')\n",
    "hindi_text_path = os.path.join(hindi_corpus, 'miscellaneous/gandhi/main.txt')\n",
    "hindi_text = open(hindi_text_path,'r', encoding='utf-8').read()\n",
    "tokenizer = TokenizeSentence('hindi')\n",
    "hindi_text_tokenize = tokenizer.tokenize(hindi_text)\n",
    "print(hindi_text_tokenize[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford Segmenter\n",
    "\n",
    "Requires `torch`. So will only run on Mac. But i hear it's a pretty good package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3.5.1\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPTokenizer\u001b[0m instead.'\n",
      "  \n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\n  NLTK was unable to find stanford-segmenter.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-segmenter.jar, see:\n    <https://nlp.stanford.edu/software>\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0e90c200446b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstanford_segmenter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStanfordSegmenter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mseg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStanfordSegmenter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mseg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'zh'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mu'这是斯坦福中文分词器测试'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msegment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3.5.1\\lib\\site-packages\\nltk\\tokenize\\stanford_segmenter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_to_jar, path_to_slf4j, java_class, path_to_model, path_to_dict, path_to_sihan_corpora_dict, sihan_post_processing, keep_whitespaces, encoding, options, verbose, java_options)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0msearchpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         )\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_to_slf4j\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda3.5.1\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_jar\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    852\u001b[0m     return next(\n\u001b[0;32m    853\u001b[0m         find_jar_iter(\n\u001b[1;32m--> 854\u001b[1;33m             \u001b[0mname_pattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    855\u001b[0m         )\n\u001b[0;32m    856\u001b[0m     )\n",
      "\u001b[1;32md:\\Anaconda3.5.1\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    838\u001b[0m             )\n\u001b[0;32m    839\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\n  NLTK was unable to find stanford-segmenter.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-segmenter.jar, see:\n    <https://nlp.stanford.edu/software>\n==========================================================================="
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.stanford_segmenter import StanfordSegmenter\n",
    "seg = StanfordSegmenter()\n",
    "seg.default_config('zh')\n",
    "sent = u'这是斯坦福中文分词器测试'\n",
    "print(seg.segment(sent))\n",
    "#这 是 斯坦福 中文 分词器 测试\n",
    "\n",
    "seg.default_config('hi')\n",
    "sent = u'भला क्या-क्या छुपाओ तुम'\n",
    "print(seg.segment(sent.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word alignment models\n",
    "\n",
    "The goal is the translation of a text given in some language F into a target language E, using a bilingual corpus.\n",
    "\n",
    "Finding a bilingual parallel corpus is *part* of the assignment. *Find one*, no excuses. The christian bible is the most translated book, and it *is* available in hindi and chinese.\n",
    "\n",
    "Once you have it, the main problem is **word alignment**. You have sentences in hindi/chinese mapped to english, but how do you map the words? There are not aligned one by one!\n",
    "\n",
    "How do you define the correspondence between the words of the hindi/chinese sentence with the words of the English\n",
    "sentence? You could start by assuming a pairwise dependence and consider *all possible word pairs*\n",
    "$(f_j, e_i)$ for a given sentence pair $i$. You can further constrain the model by assigning each hindi/chinese\n",
    "word to exactly *one* English word. In the code above, I show you how you can segment chinese and hindi words.\n",
    "\n",
    "In the graph below, in keeping with the language that we started discussing POS-tagging with (german), we see that word alignment is not a perfect diagonal line!\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/german-word-alignment.png\" width=400 />\n",
    "</center>\n",
    "\n",
    "And it's probably going to be *worse* for hindi and chinese. But I'm willing to bet that if you catalog all possible word alignment graphs for sentence word alignments, there are only going to be a few graphs for each language pair, and that these graphs are going to be *tightly coupled* with the POS tags of the original sentence. So, a good deep learning problem would be, starting from POS tags, to classify alignment graphs like the one above with sentence POS tags.\n",
    "\n",
    "Another strategy would be to look for the POS tag decompositions in the source and target languages and identify POS tag to POS tag in order to align the words.\n",
    "\n",
    "Read [this](https://www.aclweb.org/anthology/C96-2141.pdf) paper for a glimpse on alignment models.\n",
    "\n",
    "I think that for the purpose of this homework, the simplest strategy is to use a hindi/chinese to english word-to-word dictionary to identify all possible translations of a word from hindi/chinese into english, and then to locate the translation in the target sentence.\n",
    "\n",
    "If you cannot pip-install a local dictionary, I'm sure there are library APIs to google translate.\n",
    "\n",
    "Let me know if you need anything else. You have a *month*, so the due date is 20 March, but if you think starting this homework on march 17 is a good idea..\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"https://i.pinimg.com/originals/e6/6f/44/e66f44a722d283c20d985e13056c2577.gif\" width=300 />\n",
    "</center>\n",
    "\n",
    "If you enjoy NLP, there is going to be a new kind of class this summer 1, a *research* class, where I'm going to teach you how to do research, and we're going to do research in NLP. We'll study papers and investigate the state of the art in NLP. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
