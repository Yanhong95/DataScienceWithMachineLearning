{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jieba\n",
      "  Downloading https://files.pythonhosted.org/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2MB)\n",
      "Building wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py): started\n",
      "  Building wheel for jieba (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba: filename=jieba-0.42.1-cp37-none-any.whl size=19314482 sha256=fda769ca33edd02fbe1f9fee4466a69a7c3f182416f1c0292bd1e119f5610ba5\n",
      "  Stored in directory: C:\\Users\\liaoz\\AppData\\Local\\pip\\Cache\\wheels\\af\\e4\\8e\\5fdd61a6b45032936b8f9ae2044ab33e61577950ce8e0dec29\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jieba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting translate\n",
      "  Downloading https://files.pythonhosted.org/packages/85/b2/2ea329a07bbc0c7227eef84ca89ffd6895e7ec237d6c0b26574d56103e53/translate-3.5.0-py2.py3-none-any.whl\n",
      "Collecting pre-commit (from translate)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/5d/626c6de070f37b09af7b8952c914532d81e0c6cfaf2784ac852a5c449aa5/pre_commit-2.1.0-py2.py3-none-any.whl (171kB)\n",
      "Requirement already satisfied: click in c:\\users\\liaoz\\anaconda3\\lib\\site-packages (from translate) (7.0)\n",
      "Collecting tox (from translate)\n",
      "  Downloading https://files.pythonhosted.org/packages/3a/21/aa738f33db84be2caf89fae9d868320f7fc004329dd681cec4056d08cf75/tox-3.14.5-py2.py3-none-any.whl (81kB)\n",
      "Requirement already satisfied: lxml in c:\\users\\liaoz\\anaconda3\\lib\\site-packages (from translate) (4.4.1)\n",
      "Requirement already satisfied: requests in c:\\users\\liaoz\\anaconda3\\lib\\site-packages (from translate) (2.22.0)\n",
      "Collecting cfgv>=2.0.0 (from pre-commit->translate)\n",
      "  Downloading https://files.pythonhosted.org/packages/2a/b8/1b9626e940bf80cc5a19a19e6b4c99282aa88f438cbcd2d86d7a2a666974/cfgv-3.0.0-py2.py3-none-any.whl\n",
      "Collecting toml (from pre-commit->translate)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/12/ced7105d2de62fa7c8fb5fce92cc4ce66b57c95fb875e9318dba7f8c5db0/toml-0.10.0-py2.py3-none-any.whl\n",
      "Collecting identify>=1.0.0 (from pre-commit->translate)\n",
      "  Downloading https://files.pythonhosted.org/packages/73/08/ebd9cd04a6741b2cdf9a634170070d16085a38c2041e11a9b634c1cef623/identify-1.4.11-py2.py3-none-any.whl (97kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\liaoz\\anaconda3\\lib\\site-packages (from pre-commit->translate) (0.23)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\liaoz\\anaconda3\\lib\\site-packages (from pre-commit->translate) (5.1.2)\n",
      "Collecting virtualenv>=15.2 (from pre-commit->translate)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/dd/e43866ac0d25cd1ecbde0814f6cd7372ea257fbdf443dc9f0a01740da93d/virtualenv-20.0.4-py2.py3-none-any.whl (4.6MB)\n",
      "Collecting nodeenv>=0.11.1 (from pre-commit->translate)\n",
      "  Downloading https://files.pythonhosted.org/packages/08/43/86ff33286c83f7b5e8903c32db01fe122c5e8a9d8dc1067dcaa9be54a033/nodeenv-1.3.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: filelock<4,>=3.0.0 in c:\\users\\liaoz\\anaconda3\\lib\\site-packages (from tox->translate) (3.0.12)\n",
      "Requirement already satisfied: colorama>=0.4.1; platform_system == \"Windows\" in c:\\users\\liaoz\\anaconda3\\lib\\site-packages (from tox->translate) (0.4.1)\n",
      "Requirement already satisfied: py<2,>=1.4.17 in c:\\users\\liaoz\\anaconda3\\lib\\site-packages (from tox->translate) (1.8.0)\n",
      "Collecting six<2,>=1.14.0 (from tox->translate)\n",
      "  Downloading https://files.pythonhosted.org/packages/65/eb/1f97cb97bfc2390a276969c6fae16075da282f5058082d4cb10c6c5c1dba/six-1.14.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: packaging>=14 in c:\\users\\liaoz\\anaconda3\\lib\\site-packages (from tox->translate) (19.2)\n",
      "Requirement already satisfied: pluggy<1,>=0.12.0 in c:\\users\\liaoz\\anaconda3\\lib\\site-packages (from tox->translate) (0.13.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\liaoz\\anaconda3\\lib\\site-packages (from requests->translate) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\liaoz\\anaconda3\\lib\\site-packages (from requests->translate) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\liaoz\\anaconda3\\lib\\site-packages (from requests->translate) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\liaoz\\anaconda3\\lib\\site-packages (from requests->translate) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\liaoz\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->translate) (0.6.0)\n",
      "Collecting distlib<1,>=0.3.0 (from virtualenv>=15.2->pre-commit->translate)\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/29/694a3a4d7c0e1aef76092e9167fbe372e0f7da055f5dcf4e1313ec21d96a/distlib-0.3.0.zip (571kB)\n",
      "Collecting appdirs<2,>=1.4.3 (from virtualenv>=15.2->pre-commit->translate)\n",
      "  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\liaoz\\anaconda3\\lib\\site-packages (from packaging>=14->tox->translate) (2.4.2)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\liaoz\\anaconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->pre-commit->translate) (7.2.0)\n",
      "Building wheels for collected packages: distlib\n",
      "  Building wheel for distlib (setup.py): started\n",
      "  Building wheel for distlib (setup.py): finished with status 'done'\n",
      "  Created wheel for distlib: filename=distlib-0.3.0-cp37-none-any.whl size=340433 sha256=4d567002a292c57aa53285b98bde41a41891d4d86127a393d5fdcfb023e0a45f\n",
      "  Stored in directory: C:\\Users\\liaoz\\AppData\\Local\\pip\\Cache\\wheels\\6e\\e8\\db\\c73dae4867666e89ba3cfbc4b5c092446f0e584eda6f409cbb\n",
      "Successfully built distlib\n",
      "Installing collected packages: cfgv, toml, identify, distlib, appdirs, six, virtualenv, nodeenv, pre-commit, tox, translate\n",
      "  Found existing installation: six 1.12.0\n",
      "    Uninstalling six-1.12.0:\n",
      "      Successfully uninstalled six-1.12.0\n",
      "Successfully installed appdirs-1.4.3 cfgv-3.0.0 distlib-0.3.0 identify-1.4.11 nodeenv-1.3.5 pre-commit-2.1.0 six-1.14.0 toml-0.10.0 tox-3.14.5 translate-3.5.0 virtualenv-20.0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: astroid 2.3.1 requires typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\", which is not installed.\n",
      "ERROR: astroid 2.3.1 has requirement six==1.12, but you'll have six 1.14.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.core.display import HTML\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict, namedtuple, OrderedDict\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n",
    "import os\n",
    "from io import BytesIO\n",
    "from itertools import chain\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_counts(tags, words):\n",
    "    d = defaultdict(lambda: defaultdict(int))\n",
    "    for tag, word in zip(tags, words):\n",
    "        d[tag][word] += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translate import Translator\n",
    "translator= Translator(from_lang=\"chinese\",to_lang=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today\n"
     ]
    }
   ],
   "source": [
    "translation=translator.translate(\"今天\")\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'cut'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-257-276ceeb870e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0min_str\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'''我今天吃苹果了'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpeg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mresult1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'generator' object has no attribute 'cut'"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as peg\n",
    "in_str='''我今天吃苹果了'''\n",
    "words=peg.cut(in_str)\n",
    "result1=\" \"\n",
    "tags=\" \"\n",
    "for word,flag in words:\n",
    "   # temp=\"%s_%s \"%(word,flag)\n",
    "\n",
    "    translation=translator.translate(word)\n",
    "    result1=result1+translation+\" \"\n",
    "    tag=\"%s /\"%flag\n",
    "    tags=tags+tag\n",
    "   # word_counts = pair_counts(tag,words)\n",
    "print(result1)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Read tagged sentence data\"\"\"\n",
    "    with open(filename, 'r',encoding=\"utf-8\") as f:\n",
    "        sentence_lines = [l.split(\"\\n\") for l in f.read().split(\"\\n\\n\")]\n",
    "    return OrderedDict(((s[0], Sentence(*zip(*[l.strip().split(\"\\t\")\n",
    "                        for l in s[1:]]))) for s in sentence_lines if s[0]))\n",
    "\n",
    "def read_tags(filename):\n",
    "    \"\"\"Read a list of word tag classes\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        tags = f.read().split(\"\\n\")\n",
    "    return frozenset(tags)\n",
    "\n",
    "Sentence = namedtuple(\"Sentence\", \"words tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('\\ufeff1',\n",
       "              Sentence(words=('我们的', '班', '有', '一个', '很', '优秀的', '同学', '，', '她', '不光', '学习', '是', '我们', '班的', '第一名', '，', '而且', '在', '我们', '班', '还是', '经常', '受', '表扬的', '人物', '，', '她', '就是', '我们', '班的', '夏宇', '，', '她', '是', '一个', '特别', '优秀的', '学生', '，', '每年的', '三好', '学生', '，', '她', '还', '很', '热爱', '班', '集体', '。'), tags=('our', 'class', 'have', 'a', 'very', 'good', 'classmate', ',', 'she', 'not only', 'study', 'is', 'our', 'class', 'first-class', ',', 'and', 'in', 'our', 'class', 'is', 'usually', 'been', 'Praise', 'character', ',', 'she', 'is', 'our', 'class', 'xiayu', ',', 'she', 'is', 'a', 'very', 'good', 'student', ',', 'annual', 'good', 'student', ',', 'she', 'also', 'very', 'like', 'class', 'collective', '.'))),\n",
       "             ('2',\n",
       "              Sentence(words=('昨天', '放学的', '时候', '，', '同学们', '听到', '铃声', '，', '一窝蜂的', '都', '出去', '，', '她', '忽然', '停下', '脚步', '，', '我', '顺着', '她的', '目光', '，', '看见', '教室', '里', '非常', '杂乱', '，', '地上', '还有', '一大堆', '同学', '留下的', '纸屑', '。'), tags=('yesterday', 'after school', 'time', ',', 'student', 'hear', 'ring bell', ',', 'a lot', 'all', 'get out', ',', 'she', 'suddently', 'stop', 'pace', ',', 'I', 'along', 'hers', 'eye', ',', 'see', 'class', 'in', 'very', 'massly', ',', 'ground', 'also', 'a load of', 'student', 'left', 'paper', '.'))),\n",
       "             ('3',\n",
       "              Sentence(words=('我的', '好', '朋友', '看了看', '之后', '，', '想了想', '说', '可能', '是', '他们', '着急', '出去', '玩', '，', '没来得及', '收拾', '，', '我们', '来', '收拾吧', '说完', '，', '和', '我', '一起', '打扫', '教室', '。'), tags=('my', 'good', 'friend', 'look', 'after', ',', 'think', 'speak', 'probably', 'is', 'they', 'hurry', 'go out', 'play', ',', 'not too late', 'tidy', ',', 'we', \"let's\", 'clean', 'after', ',', 'with', 'I', 'together', 'clean', 'classroom', '.'))),\n",
       "             ('4',\n",
       "              Sentence(words=('我们', '扫地', '、', '摆', '小桌子', '、', '擦', '黑板', '，', '非常', '忙'), tags=('we', 'clean', ',', 'place', 'table', ',', 'wipe', 'blackboard', ',', 'very', 'busy'))),\n",
       "             ('5',\n",
       "              Sentence(words=('过了', '一会儿', '，', '终于', '打扫', '完了', '。'), tags=('after', 'a while', ',', 'finally', 'clean', 'finish', '.'))),\n",
       "             ('6',\n",
       "              Sentence(words=('夏宇', '把', '垃圾', '装在', '一个', '袋子', '，', '和', '我', '走出', '教室', '，', '关好', '门', '，', '去', '扔', '垃圾', '。'), tags=('xiayu', 'place', 'garbage', 'in', 'a', 'bag', ',', 'with', 'me', 'go out', 'class', ',', 'close', 'door', ',', 'go', 'throw', 'garbage', '.'))),\n",
       "             ('7',\n",
       "              Sentence(words=('她', '由于', '跑得', '太', '快', '，', '不小心', '摔倒了', '。'), tags=('she', 'because', 'run', 'too', 'fast', ',', 'incaution', 'fell', '.'))),\n",
       "             ('8',\n",
       "              Sentence(words=('我', '远远地', '看见', '她', '脸上', '带着', '痛苦的', '表情', '站了', '起来', '，', '揉了揉', '膝盖', '，', '坚持', '把', '垃圾', '扔掉', '，', '我们', '才', '收拾', '东西', '回', '家', '。'), tags=('I', 'far away', 'see', 'she', 'face', 'with', 'painful', 'expression', 'stand', 'up', ',', 'knead', 'knee', ',', 'insist', 'hold', 'garbage', 'throw away', ',', 'we', 'just', 'clean', 'thing', 'back', 'home', '.'))),\n",
       "             ('9',\n",
       "              Sentence(words=('这', '就是', '我们', '班的', '优秀', '同学', ',', '我', '也要', '向', '她', '学习', '，', '不仅', '要', '好好', '学习', '，', '更要', '爱护', '我们的', '班', '集体', '，', '因为', '班', '集体', '就是', '我们的', '家', '。'), tags=('this', 'is', 'our', 'class', 'good', 'student', ',', 'I', 'also', 'from', 'her', 'study', ',', 'not only', 'want', 'well', 'study', ',', 'more', 'care', 'our', 'class', 'collective', ',', 'because', 'class', 'collective', 'is', 'our', 'home', '.'))),\n",
       "             ('10',\n",
       "              Sentence(words=('三月', '到了', '，', '春', '姑娘', '也', '悄悄的', '来到了', '我们的', '身边', '，', '她', '轻手轻脚', '，', '悄悄的', '走着', '，', '浑身', '散发着', '清香', '，', '她', '羞涩地', '走着', '。'), tags=('march', 'arrived', ',', 'spring', 'girl', 'also', 'quietly', 'arrive', 'our', 'side', ',', 'she', 'lightly', ',', 'silent', 'walking', ',', 'whole body', 'exude', 'fragrance', ',', 'she', 'shyly', 'walking', '.'))),\n",
       "             ('11',\n",
       "              Sentence(words=('有一天', '，', '妈妈', '叫', '我', '去', '买', '东西', '，', '我', '开心的', '说', '好的', '妈妈', '我', '知道了', '。'), tags=('one day', ',', 'mom', 'let', 'me', 'go', 'buy', 'something', ',', 'I', 'happily', 'said', 'Okay', 'mom', 'I', 'know', '.'))),\n",
       "             ('12',\n",
       "              Sentence(words=('说完', '我', '就去', '买', '东西了', '。'), tags=('after that', 'I', 'just', 'go', 'shopping', '.')))])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagset = read_tags(\"data/brown/aa.txt\")\n",
    "sentences = read_data(\"data/brown/bb.txt\")\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(namedtuple(\"_Dataset\", \"sentences keys vocab X tagset Y training_set testing_set N stream\")):\n",
    "    def __new__(cls, tagfile, datafile, train_test_split=0.8, seed=112890):\n",
    "        tagset = read_tags(tagfile)\n",
    "        sentences = read_data(datafile)\n",
    "        keys = tuple(sentences.keys())\n",
    "        wordset = frozenset(chain(*[s.words for s in sentences.values()]))\n",
    "        word_sequences = tuple([sentences[k].words for k in keys])\n",
    "        tag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "        N = sum(1 for _ in chain(*(s.words for s in sentences.values())))\n",
    "        \n",
    "        # split data into train/test sets\n",
    "        _keys = list(keys)\n",
    "        if seed is not None: random.seed(seed)\n",
    "        random.shuffle(_keys)\n",
    "        split = int(train_test_split * len(_keys))\n",
    "        training_data = Subset(sentences, _keys[:split])\n",
    "        testing_data = Subset(sentences, _keys[split:])\n",
    "        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "        return super().__new__(cls, dict(sentences), keys, wordset, word_sequences, tagset,\n",
    "                               tag_sequences, training_data, testing_data, N, stream.__iter__)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentences.items())\n",
    "    \n",
    "    \n",
    "class Subset(namedtuple(\"BaseSet\", \"sentences keys vocab X tagset Y N stream\")):\n",
    "    def __new__(cls, sentences, keys):\n",
    "        word_sequences = tuple([sentences[k].words for k in keys])\n",
    "        tag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "        wordset = frozenset(chain(*word_sequences))\n",
    "        tagset = frozenset(chain(*tag_sequences))\n",
    "        N = sum(1 for _ in chain(*(sentences[k].words for k in keys)))\n",
    "        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "        return super().__new__(cls, {k: sentences[k] for k in keys}, keys, wordset, word_sequences,\n",
    "                               tagset, tag_sequences, N, stream.__iter__)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentences.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 sentences in the corpus.\n",
      "There are 9 sentences in the training set.\n",
      "There are 3 sentences in the testing set.\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(\"data/brown/aa.txt\", \"data/brown/bb.txt\", train_test_split=0.8)\n",
    "\n",
    "print(\"There are {} sentences in the corpus.\".format(len(data)))\n",
    "print(\"There are {} sentences in the training set.\".format(len(data.training_set)))\n",
    "print(\"There are {} sentences in the testing set.\".format(len(data.testing_set)))\n",
    "\n",
    "assert len(data) == len(data.training_set) + len(data.testing_set), \\\n",
    "       \"The number of sentences in the training set + testing set should sum to the number of sentences in the corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 265 samples of 152 unique words in the corpus.\n",
      "There are 159 samples of 103 unique words in the training set.\n",
      "There are 106 samples of 59 unique words in the testing set.\n",
      "There are 49 words in the test set that are missing in the training set.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are a total of {} samples of {} unique words in the corpus.\"\n",
    "      .format(data.N, len(data.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the training set.\"\n",
    "      .format(data.training_set.N, len(data.training_set.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the testing set.\"\n",
    "      .format(data.testing_set.N, len(data.testing_set.vocab)))\n",
    "print(\"There are {} words in the test set that are missing in the training set.\"\n",
    "      .format(len(data.testing_set.vocab - data.training_set.vocab)))\n",
    "\n",
    "assert data.N == data.training_set.N + data.testing_set.N, \\\n",
    "       \"The number of training + test samples should sum to the total number of samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 2\n",
      "words:\n",
      "\t('昨天', '放学的', '时候', '，', '同学们', '听到', '铃声', '，', '一窝蜂的', '都', '出去', '，', '她', '忽然', '停下', '脚步', '，', '我', '顺着', '她的', '目光', '，', '看见', '教室', '里', '非常', '杂乱', '，', '地上', '还有', '一大堆', '同学', '留下的', '纸屑', '。')\n",
      "tags:\n",
      "\t('yesterday', 'after school', 'time', ',', 'student', 'hear', 'ring bell', ',', 'a lot', 'all', 'get out', ',', 'she', 'suddently', 'stop', 'pace', ',', 'I', 'along', 'hers', 'eye', ',', 'see', 'class', 'in', 'very', 'massly', ',', 'ground', 'also', 'a load of', 'student', 'left', 'paper', '.')\n"
     ]
    }
   ],
   "source": [
    "key = '2'\n",
    "print(\"Sentence: {}\".format(key))\n",
    "print(\"words:\\n\\t{!s}\".format(data.sentences[key].words))\n",
    "print(\"tags:\\n\\t{!s}\".format(data.sentences[key].tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ('我们的', '班', '有', '一个', '很', '优秀的', '同学', '，', '她', '不光', '学习', '是', '我们', '班的', '第一名', '，', '而且', '在', '我们', '班', '还是', '经常', '受', '表扬的', '人物', '，', '她', '就是', '我们', '班的', '夏宇', '，', '她', '是', '一个', '特别', '优秀的', '学生', '，', '每年的', '三好', '学生', '，', '她', '还', '很', '热爱', '班', '集体', '。')\n",
      "\n",
      "Labels 1: ('our', 'class', 'have', 'a', 'very', 'good', 'classmate', ',', 'she', 'not only', 'study', 'is', 'our', 'class', 'first-class', ',', 'and', 'in', 'our', 'class', 'is', 'usually', 'been', 'Praise', 'character', ',', 'she', 'is', 'our', 'class', 'xiayu', ',', 'she', 'is', 'a', 'very', 'good', 'student', ',', 'annual', 'good', 'student', ',', 'she', 'also', 'very', 'like', 'class', 'collective', '.')\n",
      "\n",
      "Sentence 2: ('昨天', '放学的', '时候', '，', '同学们', '听到', '铃声', '，', '一窝蜂的', '都', '出去', '，', '她', '忽然', '停下', '脚步', '，', '我', '顺着', '她的', '目光', '，', '看见', '教室', '里', '非常', '杂乱', '，', '地上', '还有', '一大堆', '同学', '留下的', '纸屑', '。')\n",
      "\n",
      "Labels 2: ('yesterday', 'after school', 'time', ',', 'student', 'hear', 'ring bell', ',', 'a lot', 'all', 'get out', ',', 'she', 'suddently', 'stop', 'pace', ',', 'I', 'along', 'hers', 'eye', ',', 'see', 'class', 'in', 'very', 'massly', ',', 'ground', 'also', 'a load of', 'student', 'left', 'paper', '.')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accessing words with Dataset.X and tags with Dataset.Y \n",
    "for i in range(2):    \n",
    "    print(\"Sentence {}:\".format(i + 1), data.X[i])\n",
    "    print()\n",
    "    print(\"Labels {}:\".format(i + 1), data.Y[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stream (word, tag) pairs:\n",
      "\n",
      "\t ('我们的', 'our')\n",
      "\t ('班', 'class')\n",
      "\t ('有', 'have')\n",
      "\t ('一个', 'a')\n",
      "\t ('很', 'very')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStream (word, tag) pairs:\\n\")\n",
    "for i, pair in enumerate(data.stream()):\n",
    "    print(\"\\t\", pair)\n",
    "    if i > 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['昨天', '放学的', '时候', '，'], ['yesterday', 'after school', 'time', ','])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [word for i, (word, tag) in enumerate(data.training_set.stream())]\n",
    "tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\n",
    "words[0:4], tags[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_counts(tags, words):\n",
    "    d = defaultdict(lambda: defaultdict(int))\n",
    "    for tag, word in zip(tags, words):\n",
    "        d[tag][word] += 1\n",
    "    return d\n",
    "        \n",
    "word_counts = pair_counts(words, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfc_table = dict((word, max(tags.keys(), key=lambda key: tags[key])) for word, tags in word_counts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_counts(tags, words):\n",
    "    d = defaultdict(lambda: defaultdict(int))\n",
    "    for tag, word in zip(tags, words):\n",
    "        d[tag][word] += 1\n",
    "    return d\n",
    "        \n",
    "word_counts = pair_counts(words, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfc_table = dict((word, max(tags.keys(), key=lambda key: tags[key])) for word, tags in word_counts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "昨天 yesterday\n",
      "放学的 after school\n",
      "时候 time\n",
      "， ,\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for key, value in mfc_table.items():\n",
    "    print(key, value)\n",
    "    i += 1\n",
    "    if i > 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "FakeState = namedtuple('FakeState', 'name')\n",
    "\n",
    "class MFCTagger:\n",
    "    missing = FakeState(name = '<MISSING>')\n",
    "    \n",
    "    def __init__(self, table):\n",
    "        self.table = defaultdict(lambda: MFCTagger.missing)\n",
    "        self.table.update({word: FakeState(name=tag) for word, tag in table.items()})\n",
    "        \n",
    "    def viterbi(self, seq):\n",
    "        \"\"\"This method simplifies predictions by matching the Pomegranate viterbi() interface\"\"\"\n",
    "        return 0., list(enumerate([\"<start>\"] + [self.table[w] for w in seq] + [\"<end>\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfc_model = MFCTagger(mfc_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown(sequence):\n",
    "    return [w if w in data.training_set.vocab else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding(X, model):    \n",
    "    _, state_path = model.viterbi(replace_unknown(X))\n",
    "    return [state[1].name for state in state_path[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeff1', '10', '9']"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.testing_set.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Key: 10\n",
      "\n",
      "Sentence: ('三月', '到了', '，', '春', '姑娘', '也', '悄悄的', '来到了', '我们的', '身边', '，', '她', '轻手轻脚', '，', '悄悄的', '走着', '，', '浑身', '散发着', '清香', '，', '她', '羞涩地', '走着', '。')\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['<MISSING>', '<MISSING>', ',', '<MISSING>', '<MISSING>', '<MISSING>', '<MISSING>', '<MISSING>', '<MISSING>', '<MISSING>', ',', 'she', '<MISSING>', ',', '<MISSING>', '<MISSING>', ',', '<MISSING>', '<MISSING>', '<MISSING>', ',', 'she', '<MISSING>', '<MISSING>', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('march', 'arrived', ',', 'spring', 'girl', 'also', 'quietly', 'arrive', 'our', 'side', ',', 'she', 'lightly', ',', 'silent', 'walking', ',', 'whole body', 'exude', 'fragrance', ',', 'she', 'shyly', 'walking', '.')\n",
      "\n",
      "\n",
      "Sentence Key: 9\n",
      "\n",
      "Sentence: ('这', '就是', '我们', '班的', '优秀', '同学', ',', '我', '也要', '向', '她', '学习', '，', '不仅', '要', '好好', '学习', '，', '更要', '爱护', '我们的', '班', '集体', '，', '因为', '班', '集体', '就是', '我们的', '家', '。')\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['<MISSING>', '<MISSING>', 'we', '<MISSING>', '<MISSING>', 'student', '<MISSING>', 'I', '<MISSING>', '<MISSING>', 'she', '<MISSING>', ',', '<MISSING>', '<MISSING>', '<MISSING>', '<MISSING>', ',', '<MISSING>', '<MISSING>', '<MISSING>', '<MISSING>', '<MISSING>', ',', '<MISSING>', '<MISSING>', '<MISSING>', '<MISSING>', '<MISSING>', 'home', '.']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('this', 'is', 'our', 'class', 'good', 'student', ',', 'I', 'also', 'from', 'her', 'study', ',', 'not only', 'want', 'well', 'study', ',', 'more', 'care', 'our', 'class', 'collective', ',', 'because', 'class', 'collective', 'is', 'our', 'home', '.')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in data.testing_set.keys[1:]:\n",
    "    print(\"Sentence Key: {}\\n\".format(key))\n",
    "    print(\"Sentence: {}\\n\".format(data.sentences[key].words))\n",
    "    print(\"Predicted labels:\\n-----------------\")\n",
    "    print(simplify_decoding(data.sentences[key].words, mfc_model))\n",
    "    print()\n",
    "    print(\"Actual labels:\\n--------------\")\n",
    "    print(data.sentences[key].tags)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, Y, model):\n",
    "    \n",
    "    correct = total_predictions = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        \n",
    "        # The model.viterbi call in simplify_decoding will return None if the HMM\n",
    "        # raises an error (for example, if a test sentence contains a word that\n",
    "        # is out of vocabulary for the training set). Any exception counts the\n",
    "        # full sentence as an error (which makes this a conservative estimate).\n",
    "        try:\n",
    "            most_likely_tags = simplify_decoding(observations, model)\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "        except:\n",
    "            pass\n",
    "        total_predictions += len(observations)\n",
    "    return correct / total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy mfc_model: 93.71%\n",
      "testing accuracy mfc_model: 29.25%\n"
     ]
    }
   ],
   "source": [
    "mfc_training_acc = accuracy(data.training_set.X, data.training_set.Y, mfc_model)\n",
    "print(\"training accuracy mfc_model: {:.2f}%\".format(100 * mfc_training_acc))\n",
    "\n",
    "mfc_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, mfc_model)\n",
    "print(\"testing accuracy mfc_model: {:.2f}%\".format(100 * mfc_testing_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
